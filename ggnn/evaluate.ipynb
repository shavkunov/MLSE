{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import sparse\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GatedGraphsDataset(Dataset):\n",
    "    def __init__(self, file, label_from=0, label_to=-1, m=None, d=None):\n",
    "        self.ids = torch.load(os.path.join(file, \"ids.pkl\"))\n",
    "        self.indexes = torch.load(os.path.join(file, \"indexes.pkl\"))\n",
    "        self.tokens = torch.load(os.path.join(file, \"tokens.pkl\"))\n",
    "        self.types = torch.load(os.path.join(file, \"types.pkl\"))\n",
    "        self.labels = torch.load(os.path.join(file, \"labels.pkl\"))[:,label_from:label_to]\n",
    "        self.size = len(self.ids)\n",
    "        self.m = m if m is not None else torch.mean(self.labels, dim=0)\n",
    "        self.d = d if d is not None else torch.std(self.labels, dim=0).clamp_min(1)\n",
    "        self.norm_labels = (self.labels - self.m) / self.d\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'indexes': self.indexes[index],\n",
    "                'tokens': self.tokens[index],\n",
    "                'types': self.types[index],\n",
    "                'labels': self.labels[index],\n",
    "                'norm_labels': self.norm_labels[index],\n",
    "                'ids': self.ids[index]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GGNN(nn.Module):\n",
    "    def __init__(self, n_tokens, n_types, n_edges, \n",
    "                 node_dim, token_dim, type_dim, annotation_dim, message_dim, \n",
    "                 n_steps):\n",
    "        super(GGNN, self).__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.n_edges = n_edges\n",
    "        self.node_dim = node_dim\n",
    "        self.message_dim = message_dim\n",
    "        self.message_generator = nn.Linear(node_dim, message_dim * n_edges)\n",
    "        self.state_generator = nn.Sequential(\n",
    "            nn.Linear(type_dim + token_dim, annotation_dim),\n",
    "            nn.ConstantPad1d((0, node_dim - annotation_dim), 0))\n",
    "        self.tokens = nn.EmbeddingBag(n_tokens, token_dim, mode='sum')\n",
    "        self.types = nn.Embedding(n_types, type_dim)\n",
    "        self.updater = nn.GRUCell(input_size=message_dim, hidden_size=node_dim)\n",
    "\n",
    "    def forward(self, var_type, node_tokens, mask, adjacency_matrix):\n",
    "        tokens = self.tokens(node_tokens, per_sample_weights=mask)\n",
    "        types = self.types(var_type)\n",
    "        state = self.state_generator(torch.cat([tokens, types], 1))\n",
    "        for j in range(self.n_steps):\n",
    "            messages_out = self.message_generator(state).view((-1, self.message_dim))\n",
    "            messages_in = sparse.mm(adjacency_matrix, messages_out)\n",
    "            state = self.updater(messages_in, state)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsPredictor(nn.Module):\n",
    "    def __init__(self, n_tokens, n_types, n_edges, node_dim, token_dim, type_dim,\n",
    "                 annotation_dim, message_dim, n_steps, n_metrics):\n",
    "        super(MetricsPredictor, self).__init__()\n",
    "        self.ggnn = GGNN(n_tokens, n_types, n_edges, node_dim, token_dim, type_dim,\n",
    "                         annotation_dim, message_dim, n_steps)\n",
    "        self.attention = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(node_dim, 1))\n",
    "        self.predictor = nn.Sequential(nn.Dropout(p=0.3), nn.Linear(node_dim, n_metrics))\n",
    "\n",
    "    def forward(self, var_type, node_tokens, mask, adjacency_matrix, lens):\n",
    "        states = self.ggnn(var_type, node_tokens, mask, adjacency_matrix)\n",
    "        data = torch.nn.utils.rnn.pad_sequence(torch.split(states, lens.tolist()), batch_first=True)\n",
    "        weight = F.softmax(self.attention(data), dim=1)\n",
    "        result = torch.sum(torch.mul(data, weight), dim=1)\n",
    "        return self.predictor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(indexes, tokens, var_types, n_edges):\n",
    "    batch_size = len(indexes)\n",
    "    shifts = torch.zeros(batch_size + 1, dtype=torch.long)\n",
    "    lens = torch.zeros(batch_size, dtype=torch.long)\n",
    "    for i, token in enumerate(tokens):\n",
    "        shifts[i + 1] = shifts[i] + len(token)\n",
    "        lens[i] = len(token)\n",
    "    n_nodes = shifts[-1].item()\n",
    "    shifted_indexes = []\n",
    "    for i, index in enumerate(indexes):\n",
    "        result = index.clone()\n",
    "        result[0, :] += shifts[i]\n",
    "        result[1, :] += shifts[i] * n_edges\n",
    "        shifted_indexes.append(result)\n",
    "    result_indexes = torch.cat(shifted_indexes, dim=1)\n",
    "    result_matrix = torch.sparse.FloatTensor(result_indexes, torch.ones(len(result_indexes[0])),\n",
    "                                             (n_nodes, n_nodes * n_edges))\n",
    "    result_tokens = torch.cat(tokens)\n",
    "    result_types = torch.cat(var_types)\n",
    "\n",
    "    result_mask = (result_tokens != 0).float()\n",
    "    result_mask /= torch.clamp_min_(torch.sum(result_mask, dim=1, keepdim=True), 1)\n",
    "    return result_matrix, result_tokens, result_mask, result_types, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(values):\n",
    "    return {\n",
    "        'indexes': [item['indexes'] for item in values],\n",
    "        'tokens': [item['tokens'] for item in values],\n",
    "        'types': [item['types'] for item in values],\n",
    "        'labels': torch.stack([item['labels'] for item in values]),\n",
    "        'norm_labels': torch.stack([item['norm_labels'] for item in values]),\n",
    "        'ids': [item['ids'] for item in values]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss, train_dataset, val_dataset, out_path, \n",
    "          batch_size, n_minibatch, n_edges, n_metrics, n_epochs, device=\"cuda\"):\n",
    "    \n",
    "    model.to(device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=collate)\n",
    "    val_loader = DataLoader(val_dataset, batch_size, shuffle=False, collate_fn=collate)\n",
    "    \n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = 0\n",
    "\n",
    "        for n_batch, sample in enumerate(train_loader):\n",
    "            matrix, tokens, mask, types, lens = combine(sample['indexes'], sample['tokens'], sample['types'],\n",
    "                                                        n_edges)\n",
    "            labels = sample['norm_labels']\n",
    "            prediction = model(types.to(device), tokens.to(device), mask.to(device),\n",
    "                                  matrix.to(device), lens.to(device))\n",
    "            loss_value = loss(prediction, labels.to(device))\n",
    "            loss_value.backward()\n",
    "            if n_batch % n_minibatch == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            total_loss += loss_value.item()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            torch.save(model.state_dict(), os.path.join(out_path, f'model_{epoch}.tmp'))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(out_path, f'opt_{epoch}.tmp'))\n",
    "            \n",
    "            total = len(val_dataset)\n",
    "            predictions = np.zeros((total, n_metrics))\n",
    "            targets = np.zeros((total, n_metrics))\n",
    "            ptr = 0\n",
    "            for sample in val_loader:\n",
    "                matrix, tokens, mask, types, heads = combine(sample['indexes'], sample['tokens'],\n",
    "                                                             sample['types'],\n",
    "                                                             n_edges)\n",
    "                labels = sample['norm_labels']\n",
    "                batched_predictions = model(types.to(device), tokens.to(device), mask.to(device),\n",
    "                                               matrix.to(device), heads.to(device))\n",
    "                batched_predictions = batched_predictions.cpu().numpy()\n",
    "                predictions[ptr:ptr + len(batched_predictions)] = batched_predictions\n",
    "                targets[ptr:ptr + len(labels)] = labels\n",
    "                ptr += len(batched_predictions)\n",
    "        print(f'Epoch {epoch}: train_loss={total_loss / len(train_dataset)}, val_r2={r2_score(targets, predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, batch_size, n_edges, n_metrics, device='cuda'):\n",
    "    with torch.no_grad():         \n",
    "        model.eval()   \n",
    "        total = len(dataset)\n",
    "        predictions = np.zeros((total, n_metrics))\n",
    "        targets = np.zeros((total, n_metrics))\n",
    "        ptr = 0\n",
    "        for sample in DataLoader(dataset, batch_size, shuffle=False, collate_fn=collate):\n",
    "            matrix, tokens, mask, types, heads = combine(sample['indexes'], sample['tokens'],\n",
    "                                                             sample['types'],\n",
    "                                                             n_edges)\n",
    "            labels = sample['norm_labels']\n",
    "            batched_predictions = model(types.to(device), tokens.to(device), mask.to(device),\n",
    "                                        matrix.to(device), heads.to(device))\n",
    "            batched_predictions = batched_predictions.cpu().numpy()\n",
    "            predictions[ptr:ptr + len(batched_predictions)] = batched_predictions\n",
    "            targets[ptr:ptr + len(labels)] = labels\n",
    "            ptr += len(batched_predictions)\n",
    "        return r2_score(targets, predictions), r2_score(targets, predictions, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = path.join('..', 'data', 'torch-graphs')\n",
    "LOGS_V1_PATH = path.join('..', 'logs-v1')\n",
    "LOGS_V2_PATH = path.join('..', 'logs-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment №1\n",
    "\n",
    "Predicting 20 method-level metrics from paper \"The Effectiveness of Supervised Machine Learning Algorithms in Predicting Software Refactoring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GatedGraphsDataset(path.join(DATA_PATH, 'train'), \n",
    "                                   label_from=0, label_to=20)\n",
    "val_dataset = GatedGraphsDataset(path.join(DATA_PATH, 'val'), \n",
    "                                 label_from=0, label_to=20, \n",
    "                                 m=train_dataset.m, d=train_dataset.d)\n",
    "test_dataset = GatedGraphsDataset(path.join(DATA_PATH, 'test'), \n",
    "                                  label_from=0, label_to=20, \n",
    "                                  m=train_dataset.m, d=train_dataset.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MetricsPredictor(3766, 713, 20, 64, 32, 16, 64, 64, 4, 20)\n",
    "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.001)\n",
    "loss_function = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss=0.004699787918190081, val_r2=0.3501567211160092\n",
      "Epoch 1: train_loss=0.003616500882581565, val_r2=0.44705622389040806\n",
      "Epoch 2: train_loss=0.0032780510332922038, val_r2=0.45037572158640177\n",
      "Epoch 3: train_loss=0.0031234110907415475, val_r2=0.5032561314972239\n",
      "Epoch 4: train_loss=0.0029210608987801177, val_r2=0.5308832557729783\n",
      "Epoch 5: train_loss=0.0027959459352800108, val_r2=0.5455376680973075\n",
      "Epoch 6: train_loss=0.0027062198484498843, val_r2=0.5666657940246606\n",
      "Epoch 7: train_loss=0.00260795406529099, val_r2=0.574898061710804\n",
      "Epoch 8: train_loss=0.0025677506286193846, val_r2=0.5809345561629747\n",
      "Epoch 9: train_loss=0.002538991559174749, val_r2=0.5968767424759064\n",
      "Epoch 10: train_loss=0.002472273406612307, val_r2=0.5979866745263036\n",
      "Epoch 11: train_loss=0.002474482629436559, val_r2=0.5902181574291406\n",
      "Epoch 12: train_loss=0.0023927530148275793, val_r2=0.5921291435816454\n",
      "Epoch 13: train_loss=0.0023364848694198293, val_r2=0.6082421526139516\n",
      "Epoch 14: train_loss=0.0023414490794921444, val_r2=0.6197935187365105\n",
      "Epoch 15: train_loss=0.0023419155102219194, val_r2=0.6102117789482583\n",
      "Epoch 16: train_loss=0.002306655614812062, val_r2=0.6193704087510126\n",
      "Epoch 17: train_loss=0.002216009451462352, val_r2=0.6268139955428614\n",
      "Epoch 18: train_loss=0.0021712238894458615, val_r2=0.6460022965191913\n",
      "Epoch 19: train_loss=0.0021463061825359197, val_r2=0.6370794260237801\n",
      "Epoch 20: train_loss=0.002085002191478545, val_r2=0.6377279216160097\n",
      "Epoch 21: train_loss=0.0021061351069491274, val_r2=0.6291355295595233\n",
      "Epoch 22: train_loss=0.0020868337302098697, val_r2=0.6603863391766314\n",
      "Epoch 23: train_loss=0.0020685135836835842, val_r2=0.6619854024582972\n",
      "Epoch 24: train_loss=0.0020120062492364936, val_r2=0.6707585275164285\n",
      "Epoch 25: train_loss=0.0019804993116396405, val_r2=0.672867455805619\n",
      "Epoch 26: train_loss=0.001959086035945852, val_r2=0.6752258375138114\n",
      "Epoch 27: train_loss=0.0019414923385264215, val_r2=0.6708226813984891\n",
      "Epoch 28: train_loss=0.0019255424248001646, val_r2=0.6699403822145641\n",
      "Epoch 29: train_loss=0.0018856388564954208, val_r2=0.6655884508273852\n",
      "Epoch 30: train_loss=0.0018915126784283085, val_r2=0.6863777185918695\n",
      "Epoch 31: train_loss=0.001862875615131068, val_r2=0.6822961708989759\n",
      "Epoch 32: train_loss=0.0018682119674378471, val_r2=0.6809385540127973\n",
      "Epoch 33: train_loss=0.0018728137660089666, val_r2=0.6836983574663964\n",
      "Epoch 34: train_loss=0.0018247187552573871, val_r2=0.6894972202342377\n",
      "Epoch 35: train_loss=0.0018286045392897142, val_r2=0.6940017609275189\n",
      "Epoch 36: train_loss=0.001800717017020894, val_r2=0.6929867566292728\n",
      "Epoch 37: train_loss=0.0018305674778016587, val_r2=0.6946950747954646\n",
      "Epoch 38: train_loss=0.0017804657988726082, val_r2=0.7115206660365926\n",
      "Epoch 39: train_loss=0.0017777491460328559, val_r2=0.7097037415873851\n",
      "Epoch 40: train_loss=0.0017562759439380346, val_r2=0.6883521398528671\n",
      "Epoch 41: train_loss=0.0017229017018468864, val_r2=0.712552120202013\n",
      "Epoch 42: train_loss=0.0017294212438064575, val_r2=0.701843734388292\n",
      "Epoch 43: train_loss=0.0017469702718552079, val_r2=0.6087133762022072\n",
      "Epoch 44: train_loss=0.0018362822878342443, val_r2=0.709760616426209\n",
      "Epoch 45: train_loss=0.0017255459109564396, val_r2=0.7192806649850291\n",
      "Epoch 46: train_loss=0.0017046382310990254, val_r2=0.7173385945667905\n",
      "Epoch 47: train_loss=0.001703628166436529, val_r2=0.7108517170410115\n",
      "Epoch 48: train_loss=0.0016774841117959064, val_r2=0.7017680988575862\n",
      "Epoch 49: train_loss=0.001739545254561305, val_r2=0.721420003354198\n",
      "Epoch 50: train_loss=0.0017152396798413491, val_r2=0.7101406613376158\n",
      "Epoch 51: train_loss=0.0016854666387751697, val_r2=0.7082541347273796\n",
      "Epoch 52: train_loss=0.0017141722624418282, val_r2=0.7088340284563979\n",
      "Epoch 53: train_loss=0.0016749847762629407, val_r2=0.7248948185145189\n",
      "Epoch 54: train_loss=0.001612227375410637, val_r2=0.7333963263581029\n",
      "Epoch 55: train_loss=0.0016736388381117528, val_r2=0.7188524240368082\n",
      "Epoch 56: train_loss=0.0016295972456515705, val_r2=0.7332276856600985\n",
      "Epoch 57: train_loss=0.0015837385869428126, val_r2=0.7409440194056448\n",
      "Epoch 58: train_loss=0.0015870339857459094, val_r2=0.7422372703487674\n",
      "Epoch 59: train_loss=0.0015833267138259395, val_r2=0.6983601396527679\n",
      "Epoch 60: train_loss=0.00162692624285121, val_r2=0.7536517869504\n",
      "Epoch 61: train_loss=0.0015500632356385347, val_r2=0.748159334464509\n",
      "Epoch 62: train_loss=0.0015782656963574452, val_r2=0.7522219233034548\n",
      "Epoch 63: train_loss=0.0015562940554112808, val_r2=0.7553550595594982\n",
      "Epoch 64: train_loss=0.0015623941845025375, val_r2=0.754482441435733\n",
      "Epoch 65: train_loss=0.0015376083762989692, val_r2=0.7694848168574977\n",
      "Epoch 66: train_loss=0.0015114841551029127, val_r2=0.7662812492639051\n",
      "Epoch 67: train_loss=0.0015046162057673902, val_r2=0.7731000279061325\n",
      "Epoch 68: train_loss=0.0014869202767758085, val_r2=0.7745288873630383\n",
      "Epoch 69: train_loss=0.001502710613457674, val_r2=0.7697899527257599\n",
      "Epoch 70: train_loss=0.001492706413846441, val_r2=0.773536071376822\n",
      "Epoch 71: train_loss=0.0014697841309540996, val_r2=0.7657792274527572\n",
      "Epoch 72: train_loss=0.001462152826107377, val_r2=0.7669966944466948\n",
      "Epoch 73: train_loss=0.0014521702277540988, val_r2=0.7703696362040816\n",
      "Epoch 74: train_loss=0.0014367996263215065, val_r2=0.7804591857942293\n",
      "Epoch 75: train_loss=0.001413654737001209, val_r2=0.776943644065303\n",
      "Epoch 76: train_loss=0.001418436278451504, val_r2=0.7646540092673801\n",
      "Epoch 77: train_loss=0.0014538606929176195, val_r2=0.7659746222317281\n",
      "Epoch 78: train_loss=0.0014403269957756896, val_r2=0.766695726352476\n",
      "Epoch 79: train_loss=0.0014237235913424063, val_r2=0.7678244582069467\n",
      "Epoch 80: train_loss=0.0014113728188605986, val_r2=0.7651686382164594\n",
      "Epoch 81: train_loss=0.0014379069163965902, val_r2=0.7660840665212023\n",
      "Epoch 82: train_loss=0.0014011476774608185, val_r2=0.7513089651509366\n",
      "Epoch 83: train_loss=0.0013931987396869803, val_r2=0.7726096423066706\n",
      "Epoch 84: train_loss=0.001379050362577555, val_r2=0.7651778825823385\n",
      "Epoch 85: train_loss=0.0014060508633985795, val_r2=0.7445794937539361\n",
      "Epoch 86: train_loss=0.0013963648905737536, val_r2=0.7637087602943715\n",
      "Epoch 87: train_loss=0.0013802349697898688, val_r2=0.7569801152608132\n",
      "Epoch 88: train_loss=0.0013798507704205123, val_r2=0.7608846907318433\n",
      "Epoch 89: train_loss=0.001387651481382704, val_r2=0.7583638264998241\n",
      "Epoch 90: train_loss=0.0013780201535728068, val_r2=0.7560743865501073\n",
      "Epoch 91: train_loss=0.0013603016714750472, val_r2=0.7629662634753485\n",
      "Epoch 92: train_loss=0.001345156922944722, val_r2=0.7777526403557111\n",
      "Epoch 93: train_loss=0.0013283790144333933, val_r2=0.7381139341169651\n",
      "Epoch 94: train_loss=0.001341919199210475, val_r2=0.7603876343245053\n",
      "Epoch 95: train_loss=0.0013533779282217222, val_r2=0.7541509187699551\n",
      "Epoch 96: train_loss=0.0013349277068360516, val_r2=0.766278482875826\n",
      "Epoch 97: train_loss=0.001346287654738995, val_r2=0.7665036423182189\n",
      "Epoch 98: train_loss=0.001348819860151404, val_r2=0.7610722716458485\n",
      "Epoch 99: train_loss=0.00138589270413694, val_r2=0.7629323742461385\n"
     ]
    }
   ],
   "source": [
    "train(predictor, optimizer, loss_function, \n",
    "      train_dataset, val_dataset, LOGS_V1_PATH,\n",
    "      128, 10, 20, 20, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7815155417707412,\n",
       " array([ 0.80132784,  0.85873349,  0.77229692,  0.8847502 ,  0.72237458,\n",
       "         0.87076254,  0.90914218,  0.86489323,  0.84928428,  0.7588517 ,\n",
       "         0.93320699,  0.61455496,  0.87934504,  0.80410059,  0.77147378,\n",
       "        -0.07314711,  0.89284019,  0.78164229,  0.88228996,  0.85158721]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.load_state_dict(torch.load(path.join(LOGS_V1_PATH, f'model_74.tmp')))\n",
    "evaluate(predictor, test_dataset, 128, 20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment №2\n",
    "Predicting 13 method-level metrics calculated by MetricsReloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MetricsPredictor(3766, 713, 20, 64, 32, 16, 64, 64, 4, 13)\n",
    "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GatedGraphsDataset(path.join(DATA_PATH, 'train'), \n",
    "                                   label_from=20, label_to=33)\n",
    "val_dataset = GatedGraphsDataset(path.join(DATA_PATH, 'val'), \n",
    "                                 label_from=20, label_to=33, \n",
    "                                 m=train_dataset.m, d=train_dataset.d)\n",
    "test_dataset = GatedGraphsDataset(path.join(DATA_PATH, 'test'), \n",
    "                                  label_from=20, label_to=33, \n",
    "                                  m=train_dataset.m, d=train_dataset.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss=0.0035109741755458587, val_r2=0.5432975417783402\n",
      "Epoch 1: train_loss=0.0025585684377230126, val_r2=0.6341584138507037\n",
      "Epoch 2: train_loss=0.0022635171184727616, val_r2=0.6614981228597719\n",
      "Epoch 3: train_loss=0.0020588345689431116, val_r2=0.6833023405987686\n",
      "Epoch 4: train_loss=0.0019307566014970412, val_r2=0.6815910878157574\n",
      "Epoch 5: train_loss=0.0018401389506510814, val_r2=0.7047077975639016\n",
      "Epoch 6: train_loss=0.0017427176320668895, val_r2=0.6873186164218281\n",
      "Epoch 7: train_loss=0.0017384150328531793, val_r2=0.744637409716666\n",
      "Epoch 8: train_loss=0.00166123433571817, val_r2=0.7513903456652083\n",
      "Epoch 9: train_loss=0.0015800072028096584, val_r2=0.766439613213781\n",
      "Epoch 10: train_loss=0.0015911463287228095, val_r2=0.7191693057313218\n",
      "Epoch 11: train_loss=0.0016550420646859378, val_r2=0.7492241402844712\n",
      "Epoch 12: train_loss=0.0015657284579050788, val_r2=0.7700433416816624\n",
      "Epoch 13: train_loss=0.0016032194850870928, val_r2=0.7606430954769287\n",
      "Epoch 14: train_loss=0.001537527405627859, val_r2=0.769709399592948\n",
      "Epoch 15: train_loss=0.0015923680299569176, val_r2=0.7605474101484415\n",
      "Epoch 16: train_loss=0.0014971650721062747, val_r2=0.7524182533154454\n",
      "Epoch 17: train_loss=0.0015277552075518208, val_r2=0.770169152939541\n",
      "Epoch 18: train_loss=0.0014510364146031892, val_r2=0.7795345227676952\n",
      "Epoch 19: train_loss=0.0014261260133712227, val_r2=0.7870227299227609\n",
      "Epoch 20: train_loss=0.001371193449114844, val_r2=0.7818597703260581\n",
      "Epoch 21: train_loss=0.0013419552948508195, val_r2=0.7902512462599844\n",
      "Epoch 22: train_loss=0.0013600970646108436, val_r2=0.797663575604592\n",
      "Epoch 23: train_loss=0.0013306123262024616, val_r2=0.8079909680261802\n",
      "Epoch 24: train_loss=0.001282707988430131, val_r2=0.8125751826599049\n",
      "Epoch 25: train_loss=0.0012998551504554041, val_r2=0.8048481353286733\n",
      "Epoch 26: train_loss=0.0012530175713666755, val_r2=0.7952427284698256\n",
      "Epoch 27: train_loss=0.0012220765862123316, val_r2=0.8096688113990795\n",
      "Epoch 28: train_loss=0.0012248551157564905, val_r2=0.8082131052636379\n",
      "Epoch 29: train_loss=0.0012103835052764379, val_r2=0.806741727296751\n",
      "Epoch 30: train_loss=0.0011829610243280341, val_r2=0.8130980206921536\n",
      "Epoch 31: train_loss=0.0012235280752754204, val_r2=0.8173850571050963\n",
      "Epoch 32: train_loss=0.0011491446743815011, val_r2=0.8123771699066182\n",
      "Epoch 33: train_loss=0.0011571240158544912, val_r2=0.8115399380472679\n",
      "Epoch 34: train_loss=0.0011390202965198904, val_r2=0.7737502436130254\n",
      "Epoch 35: train_loss=0.0013751503356928445, val_r2=0.8012566058748329\n",
      "Epoch 36: train_loss=0.0011748887041723214, val_r2=0.8141981781723893\n",
      "Epoch 37: train_loss=0.0011291932344570852, val_r2=0.8150366986244497\n",
      "Epoch 38: train_loss=0.001193387952521958, val_r2=0.8132102957451763\n",
      "Epoch 39: train_loss=0.0011267584320549257, val_r2=0.819019061725898\n",
      "Epoch 40: train_loss=0.0010762813777148146, val_r2=0.7988830521096725\n",
      "Epoch 41: train_loss=0.0010633158520070023, val_r2=0.8234520167790743\n",
      "Epoch 42: train_loss=0.0010527890759255533, val_r2=0.8225338222866992\n",
      "Epoch 43: train_loss=0.0011087905412996487, val_r2=0.8270274918852222\n",
      "Epoch 44: train_loss=0.001159912712910423, val_r2=0.8168358498219347\n",
      "Epoch 45: train_loss=0.0011303247241122845, val_r2=0.8228440790994578\n",
      "Epoch 46: train_loss=0.0010614656304992927, val_r2=0.8321907131326812\n",
      "Epoch 47: train_loss=0.0010864989314962952, val_r2=0.815860207109396\n",
      "Epoch 48: train_loss=0.0010967193086643678, val_r2=0.8158664360165638\n",
      "Epoch 49: train_loss=0.001050688517440178, val_r2=0.8221150453716941\n"
     ]
    }
   ],
   "source": [
    "train(predictor, optimizer, loss_function, \n",
    "      train_dataset, val_dataset, LOGS_V2_PATH,\n",
    "      128, 10, 20, 13, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8292503230455129,\n",
       " array([0.90125511, 0.7649991 , 0.84024208, 0.6153092 , 0.91130423,\n",
       "        0.86794405, 0.93926821, 0.85808415, 0.8296392 , 0.44913948,\n",
       "        0.91622912, 0.94355596, 0.94328431]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.load_state_dict(torch.load(path.join(LOGS_V2_PATH, f'model_46.tmp')))\n",
    "evaluate(predictor, test_dataset, 128, 20, 13)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaPython",
   "language": "python",
   "name": "condapython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
